import string
import re
def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    s = text
    if casefold:
        s = s.casefold()
    if yo2e:
        s = s.replace('—ë', '–µ').replace('–Å', '–ï')
    s = s.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
    s = s.strip()
    while '  ' in s:
        s = s.replace('  ', ' ')
    return s

def tokenize(text: str) -> list[str]:
    # The regular expression finds (and put it in var "default"):
        # 1. Words containing a hyphen (\w+-\w+)
        # 2. Numbers (\d+)
        # 3. Letter/number/underscore pattern words (\w+)
    default = r"\w+-\w+|\d+|\w+"
    #Find all pattern characters in text and return only these.
    return re.findall(default, text.lower())

def count_freq(tokens: list[str]) -> dict[str, int]:
    d = {}
    tokens_set = set(tokens)
    for key in tokens_set:
        d[key] = tokens.count(key)
    return d

def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    d = freq
    d = sorted(d.items(), key = lambda para: (-para[1], para[0]))
    return d
example_normalize = "  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "
example_tokenize = "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"
example_count_freq = ["bb","aa","bb","aa","cc"]

res_normalise = normalize(example_normalize)
res_tokenize = tokenize(example_tokenize)
res_count_freq = count_freq(example_count_freq)
res_count_top_n = top_n(res_count_freq)

print(res_normalise)    
print(res_tokenize)
print(res_count_freq)
print(res_count_top_n)